{"cells":[{"cell_type":"markdown","metadata":{"id":"6Og4DnJPrB4A","cell_id":"ad8def54b2ac47d1816015edc189f19b","deepnote_cell_type":"markdown"},"source":"# Week 7 - Support Vector Machines\n\n### Aims\n\nBy the end of this notebook you will be able to understand \n\n>* the Separable vs Non-separable data\n>* Use of different kernels and parameter tuning for SVM\n>* the model assessment for SVM\n>* the binary case for Default data \n\n\n1. [Setup](#setup)\n\n2. [Separable and Non separable Data Cases](#RBH)\n\n3. [Model assessment](#assess)\n\n4. [Default Data for Binary Example](#default)\n","block_group":"00000-34f9557d-8f15-494e-8d65-74c8ae429c5c"},{"cell_type":"markdown","metadata":{"id":"AdHUSbWsvZ7h","cell_id":"aad84c6e1a7c4f17a90f7e663edb9037","deepnote_cell_type":"markdown"},"source":"- In this WS we will be exploring the basics of support vector machine models. \n\n- We will be focusing on the most straight forward case, which is a support vector machine classifier which is provide by sklearn as the SVC model. For the details please have a look at https://scikit-learn.org/stable/modules/svm.html\n\nMain function that we are using is [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n\n**NOTE THAT**, for the simplicity we did not use any data partitioning in below for toy data examples. \nBut for the real data set (Default), we will have the data splitting procedure as a general procedure. \n\nAs usual, during workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. When completing worksheets:\n\n>- You will have tasks tagged by (CORE) and (EXTRA). \n>- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n>- Look for the ðŸ as cue to switch roles between driver and navigator.\n>- In some Exercises, you will see some hints at the bottom of questions. Some of them include fill in the blanks or commented line of codes that you need to change while running the code!\n\nInstructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given.","block_group":"dd6d3fcdaaae4a17879e609d2553cc08"},{"cell_type":"markdown","metadata":{"id":"6sVlUI4SvZ7i","cell_id":"11e552fe587a491ca541a982ba8f7c96","deepnote_cell_type":"markdown"},"source":"# 1. General Setup <a id='setup'></a>\n\n## 1.1 Packages\n\nNow lets load in the packages you wil need for this workshop.\n","block_group":"00001-645a25eb-6010-425a-88c0-ecf0093a9edc"},{"cell_type":"code","metadata":{"id":"grVNp8GrrH0g","output_cleared":true,"cell_id":"d0306d79715442bd990858287f111dfd","deepnote_cell_type":"code"},"source":"# Display plots inline\n%matplotlib inline  \n\n# Data libraries\nimport pandas as pd\nimport numpy as np\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# sklearn modules list that might be useful, maybe you do not need to use all of them\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC, LinearSVC           # SVM\nfrom sklearn.preprocessing import StandardScaler # scaling features\nfrom sklearn.preprocessing import LabelEncoder   # binary encoding\nfrom sklearn.pipeline import Pipeline            # combining classifier steps\nfrom sklearn.preprocessing import PolynomialFeatures # make PolynomialFeatures\nfrom sklearn.datasets import make_classification, make_moons  # make example data\nimport warnings # prevent warnings\nimport joblib # saving models\n\nfrom time import time\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold\nfrom scipy.stats.distributions import uniform, loguniform\nimport itertools\nfrom sklearn.model_selection import GridSearchCV, KFold\n#  from imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\n# from imblearn.metrics import classification_report_imbalanced\n\nimport re\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","block_group":"00002-d0af5d8f-8894-4c5a-b754-353993666790","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"03706ad381f74d82b184a96566929233","deepnote_cell_type":"code"},"source":"# Plotting defaults\nplt.rcParams['figure.figsize'] = (8,8)\nplt.rcParams['figure.dpi'] = 80\nplt.rcParams['lines.markersize'] = 7.5","block_group":"b55222c279864b28a728026e169951b9","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"N8_vjOPKdqLm","cell_id":"a3ef086b4c524e9ea57ff5a2e14ef7a2","deepnote_cell_type":"markdown"},"source":"##  1.2 Helper Functions\n\nBelow are helper functions we will be using in this workshop. You can create your own if you think it is necessary OR directly use already available helper functions within `sklearn library`.  \n\n- `plot_margin()`: visualization of margins in figures.\n\nYou can modify the following function based on your needs as well. These practices would be important while you are working on your project either. ","block_group":"dac0e9ac8d3d4d1492aa7b9bc99386ef"},{"cell_type":"code","metadata":{"id":"A3LgZwfrdyq7","cell_id":"3fdc9a19c5b94a3eaafab349c19c50b6","deepnote_cell_type":"code"},"source":"# About visualization of margins in figures\ndef plot_margin(model, data, x='x', y='y', cat='z', show_support_vectors = True, nx=50, ny=50):\n    # Plot the data\n    p = sns.scatterplot(x=x, y=y, hue=cat, data=data, legend=False)\n    \n    # Find the extent of x and y\n    xlim = p.get_xlim()\n    ylim = p.get_ylim()\n    \n    # Create a grid of points\n    xx = np.linspace(xlim[0]-1, xlim[1]+1, nx)\n    yy = np.linspace(ylim[0]-1, ylim[1]+1, ny)\n    YY, XX = np.meshgrid(yy, xx)\n    \n    # Calculate the label for each point in the grid\n    xy = np.c_[XX.ravel(), YY.ravel()]\n    Z = model.decision_function(xy).reshape(XX.shape)\n    \n    # plot contours of decision boundary and margins\n    p.contour(XX, YY, Z, colors='k', \n              levels=[-1, 0, 1], alpha=0.5,\n              linestyles=['--', '-', '--'])\n\n    # highlight support vectors\n    if (show_support_vectors):\n        p.scatter(model.support_vectors_[:, 0], \n                  model.support_vectors_[:, 1], s=100,\n                  linewidth=1, facecolors='none', edgecolors='k')\n\n    # Show confusion table in the title\n    p.set_title(\n        \"TN: {0}, FP: {1}, FN: {2}, TP: {3}\".format(\n            *confusion_matrix(\n                data[cat],\n                model.predict(data.drop(cat, axis=1))\n            ).flatten()\n        )\n    )\n    plt.legend(loc='lower left')\n    plt.show()","block_group":"2b657e0b293e4470abb63cee78aca004","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"d4e127686dad445284a8740f63cbbb0e","deepnote_cell_type":"markdown"},"source":" **__REMARK__**\n\n- The implementation of the SVC with sklearn requires a slightly different meaning for the parameter C. Please note that, \"Regularization parameter. \n- The strength of the regularization is inversely proportional to C. Must be strictly positive.\" and \"The C parameter trades of correct classification of training examples against maximization of the decision function's margin. \n- For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. \n- A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words C behaves as a regularization parameter in the SVM.\"","block_group":"91db149a587d4e2f835de2611487b988"},{"cell_type":"markdown","metadata":{"cell_id":"4bd0523b8ac846d3b6f8bfa4de55cdb0","deepnote_cell_type":"markdown"},"source":"### **Difference between SVC and LinearSVC**\n\nThe linear models `LinearSVC()` and `SVC(kernel='linear')` yield slightly different decision boundaries. This can be a consequence of the following differences:\n\n>- `LinearSVC` minimizes the squared hinge loss while SVC minimizes the regular hinge loss.\n>- `LinearSVC` uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while `SVC` uses the One-vs-One multiclass reduction.\n>- In terms of graphical display, note that unlike `SVC` (based on LIBSVM), `LinearSVC` (based on LIBLINEAR) does not provide the support vectors.\n\nFor further details, try to compare differences from their documentations\n\n- `SVC` with `linear` kernel selection: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n- `LinearSVC` function: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n\n**Try to focus on `SVC` usage in general for the simplicity**","block_group":"df7135b029e0441e8907a76c9dbd86ad"},{"cell_type":"markdown","metadata":{"id":"yz3bjxcbvZ7r","cell_id":"2a2e3e9fdd124d2283f3296ee2eae354","deepnote_cell_type":"markdown"},"source":"# 2 Separable Data <a id='RBH'></a>\n\nWe will begin by examining several toy data problems to explore the basics of these models. To begin we will read in data for the first example from ex1.csv\n","block_group":"00003-90709695-8746-4669-9199-fd144a6ec872"},{"cell_type":"code","metadata":{"id":"eF19U6ivvZ7r","cell_id":"979c3aa0c23140fcbaa6a4d1e2d434da","deepnote_cell_type":"code"},"source":"ex1 = pd.read_csv(\"ex1.csv\")\nex1.head()","block_group":"4dea4a49e31647ed95c87c22635eee10","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"izo4A3SSvZ7t","cell_id":"29aa776c9b36482781b4b639544ca096","deepnote_cell_type":"markdown"},"source":"We can see the that data is composed of two classes in two dimensions, and it is clear that these two classes are perfectly linearly separable","block_group":"8449ac36cd3641b4978c5f81913c0963"},{"cell_type":"code","metadata":{"id":"r3QIcTBsCZAf","colab":{"height":548,"base_uri":"https://localhost:8080/"},"outputId":"5c544249-18e4-4129-be03-fc2bc14f45cb","cell_id":"b89bbebaea964fb6b2a478ac8962d452","deepnote_cell_type":"code"},"source":"sns.scatterplot(x='x', y='y', hue='z', data=ex1, legend=False)\nplt.show()","block_group":"520d99964dd54668b63399f0541b8f1c","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"50AbKP76vZ7u","cell_id":"779f6afa96774ae9b77940b5f4221d5c","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 1 (CORE)\n\nLike the other models we've already seen, we fit the SVM by constructing our feature matrix and outcome vector and then calling the fit method for our model object;\n\n1. Separate the features and outcome in the toy dataset ex1.csv\n2. Fit a SVC model for this data set using `SVC()` function (Note that you need to change the default value of kernel and parameter C)\n3. Visualize the decision boundary and the margins using the plot_margin function we defined above.\n","block_group":"00005-da6d5f4b-bf6b-43f7-9751-a015e6a9924e"},{"cell_type":"code","metadata":{"id":"-mG-rvtrvZ7u","output_cleared":true,"cell_id":"5e8c7b6afe884988806a40f3fe4f595c","deepnote_cell_type":"code"},"source":"","block_group":"00006-4d9907ea-c7c7-4c16-aee7-5e12c3e00cc1","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"01HS3aS8vZ7u","cell_id":"16f7fc4bafe14330909e4c2aa2615811","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**\n","block_group":"00007-a3d1594c-000b-48dd-b347-dfa5410fe0a7"},{"cell_type":"markdown","metadata":{"cell_id":"ef7205e2949c40bdbc199680d0fed2dd","deepnote_cell_type":"markdown"},"source":"### ðŸš© Exercise 2  (CORE)\n\nBased on the results of previous exercise, state that\n\n- How many support vectors are there for this model?\n- How does the boundary line and the margins change as you change the value of C?","block_group":"e7e0d25701e6415eabf17b75c107ef03"},{"cell_type":"markdown","metadata":{"cell_id":"a0fa1272904d49cebc7355928c85782e","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**","block_group":"d8ab8a6476184dda80b57afdf5a5aa5e"},{"cell_type":"markdown","metadata":{"id":"AKglNm6mD_jx","cell_id":"f251ceebdf7548fbb235ca68e5b4aea2","deepnote_cell_type":"markdown"},"source":"# 3 Non-Separable Data\n\nWe will not complicate our previous example somewhat by adding two additional points from the blue A class to our data. This is available in the ex2.csv file.","block_group":"6d401ccb27d0406e82f85f36b6517729"},{"cell_type":"code","metadata":{"id":"7XEqi56tEHsB","colab":{"height":548,"base_uri":"https://localhost:8080/"},"outputId":"d9414ca1-3126-4d20-ad0f-dd5602238d03","cell_id":"c54ca771da2947c88fc9f9384feb4c2a","deepnote_cell_type":"code"},"source":"ex2 = pd.read_csv(\"ex2.csv\")\nprint(ex2.head())\n\n# To visualize\nsns.scatterplot(x='x', y='y', hue='z', data=ex2, legend=False)\nplt.show()","block_group":"ca2e1a07c3164ab783211d1bf83328fa","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"O6TMeCFgAJVt","cell_id":"86e3eeeb71b1429ba8c0b8a29bfbd6b6","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 3  (CORE)\n\n- Fit a SVC model to these data using the same code we used with example 1.\n- How does the \"fit\" of this model differ compared to the \"fit\" for example 1. Hint - make your comparison for equivalent values of C.\n- How do the boundary line and margins change as you change the value of C?\n\n\n","block_group":"3384746030f542fd88d3a51179c0fa23"},{"cell_type":"markdown","metadata":{"id":"4xhjuXdcvZ7v","cell_id":"caeebe9e651e4212acbb0b65c8fdea67","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your comments about the answer here !!!**\n","block_group":"5492bf6fe9c1489d94b85aac0777721e"},{"cell_type":"markdown","metadata":{"id":"_YZt5su8C7WZ","cell_id":"02e3f7ce92554d2293815d8fbb14b3eb","deepnote_cell_type":"markdown"},"source":"# 4  Non-linear Case\n\nNext we will look at a new data set that would seem to also fall in the non-separable category. The data set that we are using is ex3.csv now\n\n","block_group":"e3da40781aa64bd5bad09d40a346b952"},{"cell_type":"code","metadata":{"id":"jSwzAMONFJm1","colab":{"height":548,"base_uri":"https://localhost:8080/"},"outputId":"229a9a69-2e51-4ff8-b902-ac52fc25c439","cell_id":"4d73764a61d04f748c72833017fd0b79","deepnote_cell_type":"code"},"source":"# For the new data set \nex3 = pd.read_csv(\"ex3.csv\")\n\n# To visualize\nsns.scatterplot(x='x', y='y', hue='z', data=ex3, legend=False)\nplt.show()","block_group":"37f7845f3b9a4de49901136368382827","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"mvrOq4afvZ7z","cell_id":"3ed3d42cb6e04c318c5135f56d7eda12","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 4  (CORE)\n\nFor this data we will consider a simple polynomial kernel with degree 2 \n(choose first C = 1 ) and visualize the margins using `plot_margin()` again\n\nMore details on the various kernels that can be used with the SVC model are available \nhttps://scikit-learn.org/stable/modules/svm.html#svm-kernels\n\n**The kernel function can be any of the following:**\n\n- linear : $\\langle x, x'\\rangle$\n- polynomial : $(\\gamma \\langle x, x'\\rangle + r)^d$\n- rbf : $\\exp(-\\gamma \\|x-x'\\|^2)$ where  $\\gamma$ is specified by parameter gamma, must be greater than 0\n- sigmoid : $\\tanh(\\gamma \\langle x,x'\\rangle + r)$ where $r$ is specified by `coef0`","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"i2qXwFXvQbBq","cell_id":"6dbe934ca414414297027c36e5366ef5","deepnote_cell_type":"code"},"source":"","block_group":"4500ced978434eb0a3fc63f291208ac4","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"38cc3a510a904c6aa63c71ef81c81501","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**","block_group":"5a6a96293b604e118fe224b8f6f3da8a"},{"cell_type":"markdown","metadata":{"id":"TLaXXgVqGQrd","cell_id":"e429b121243e46aeb9abbd23b846c746","deepnote_cell_type":"markdown"},"source":"## 4.1 Other Kernels\n\nNext we will consider an even more complicated separation task where one class is split into two separate clusters by the second class. The data ara available as `ex4.csv`.","block_group":"5dddb3c8783745059258489953b841d0"},{"cell_type":"code","metadata":{"id":"fAZUQs23GXAl","colab":{"height":548,"base_uri":"https://localhost:8080/"},"outputId":"5634934d-e3f0-4972-8e54-d5b12da535d3","cell_id":"44506646f9c74737838ceb493fdbaea7","deepnote_cell_type":"code"},"source":"# For the new data set \nex4 = pd.read_csv(\"ex4.csv\")\n\n# To visualize\nplt.figure(figsize=(8, 8))\nsns.scatterplot(x='x', y='y', hue='z', data=ex4, legend=False)\nplt.show()","block_group":"68934d6890894426ab8bac797b530a51","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"z4NLHMUEdKxP","cell_id":"e572b9625b3c4bcc9f21ea3f8b879bd8","deepnote_cell_type":"markdown"},"source":"---\n \n### ðŸš© Exercise 5  (CORE)\n\nSet up a function for experimenting with different penalties and kernel functions for this dataset (ex4.csv). For this purpose, consider, \n\n1. **C** in $[1,5,10,50,100]$\n2. **degree** in $[2,3,4]$\n3. **kernel** in $['poly', 'rbf', 'linear'])$\n\ninside of the `SVC()` function. Note that the degree value is only used by polynomial kernel and is ignored by the linear and rbf kernels. \n\nWhat combination of parameters appears to produce the best fit? Is it easy to tell this by visual inspection alone?","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"9vTpIe932BFz","cell_id":"e98a088eacc140639da2a73a7175bb5b","deepnote_cell_type":"code"},"source":"","block_group":"0f8dc5aae99d466499c38f9b034a4cd2","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"c8524f7c42bc4559800059d1218f349d","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**","block_group":"2fce88838a1a49c187e2fc8a4c19b4cd"},{"cell_type":"markdown","metadata":{"id":"2uoEYNyw0Jfm","cell_id":"54aa0fcf05a64b1fb649982ff26ec9fc","deepnote_cell_type":"markdown"},"source":"# 5  Model Assessment <a id='assess'></a>\n\nSo far we have only inspected the various models by eye to get a sense of how well they fit our data. Since we are undertaking a classification task here we would like to be able to leverage the metrics and scoring tools we have already learned around logistic regression and related tools. The issue is that while we could generate a simple confusion matrix for our models' predictions this is somewhat limiting.\n\n**WARNING:** \n\n- By default, SVM models do not support the construction of anything like a ROC curve since the predictions are not probabilistic - i.e. labels are assigned based on which side of the separator a point falls. \n- As such, SVC models do not implement predict_proba by default\n- Just to recall, these are some metrics that we discussed before\n\n$$\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+ \\text{TN}}\n$$\n\n$$\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+ \\text{FN}}\n$$\n\n$$\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+ \\text{FP}}\n$$\n\n$$\nF1 = 2\\left(\\frac{Precision \\times Recall}{Precision + Recall}\\right)\n$$","block_group":"72522182214e4cd899b0bdf0f1695aec"},{"cell_type":"markdown","metadata":{"id":"Szq6fbGf05A9","cell_id":"7aebecbe61804647ac61b529c796bd61","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 6  (CORE)\n\nBased on the best model that you visualized above,\n\n1. Report the accuracy of the model\n2. Obtain the confusion matrix and interpret the results in terms of the quantities defined below \n","block_group":"c5c17bf22f334c099b2f6e221e5051e1"},{"cell_type":"code","metadata":{"id":"UQrDeC8O8-Nf","cell_id":"7fa68f5a5cf14d66990d20fe125b7bbb","deepnote_cell_type":"code"},"source":"","block_group":"80450026092a4a84a204b968345a8ce6","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"Ruvl0mkhF8xL","cell_id":"6bd39fddab31408a8ff42291177da7af","deepnote_cell_type":"markdown"},"source":"**!!! Add your comments about the model performance here !!!**\n","block_group":"bf31623438514c4aa693e2a966372021"},{"cell_type":"markdown","metadata":{"id":"cjuQg3ty7XYG","cell_id":"db517838131a43c0a540b40d16e3b725","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 7  (CORE)\n\n- Construct a full cross validated grid search over the parameter values: \n\n$C = np.linspace(0.1, 10, 100)$, degree = $[2,3,4]$, and kernel = $['poly', 'rbf', 'linear']$.\n\n- Which SVM model performs best? Use plot_margin to show the resulting seperator and support vectors.\n\n**Note**: Degree of the polynomial kernel function (â€˜polyâ€™) only!. Must be non-negative. Ignored by all other kernels.","block_group":"7c577c69fbb9461c90b62a0afaa63f67"},{"cell_type":"code","metadata":{"id":"AMIeAU0kjec1","hideCode":false,"hidePrompt":false,"cell_id":"6db676c38d5c405f853a6ef4eb5efdd7","deepnote_cell_type":"code"},"source":"cv = GridSearchCV(\n    SVC(),\n    param_grid = { \n        'kernel':________________, \n        'C': _______________,\n        'degree': [2,3,4]\n    },\n    cv = KFold(5, shuffle = True, random_state = 42)\n)\n\n# Fit the model on ex4 data set\n\n# Get the best model parameters and the accuracy of the model\n","block_group":"3d2cb96d86f644dca7640d67c051d2c1","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"ba9f71ccb2cf485a8bd4d4c59c56b00f","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**","block_group":"ed03fa6b741245c3b8c6737e57220f8c"},{"cell_type":"markdown","metadata":{"id":"YwCfjeYB4tVM","cell_id":"59005d77c4284ed89e5bbc26a2bd88d7","deepnote_cell_type":"markdown"},"source":"# 6 Default Data Case <a id='default'></a>\n\nThe dataset consists of 10000 individuals and whether their credit card has defaulted or not. Below is the column description: The main aim is to build the model using Logistic Regression and predict the accuracy of it. The included columns in the data set are as follows:\n\n* `default` - Whether the individual has defaulted\n\n* `student` - Whether the individual is the student\n\n* `balance` - The balance in the individual's account\n\n* `income` - Income of an individual\n\nWe read the data into python using pandas.\n","block_group":"00003-90709695-8746-4669-9199-fd144a6ec872"},{"cell_type":"code","metadata":{"id":"SlbaLT675J8g","colab":{"height":206,"base_uri":"https://localhost:8080/"},"outputId":"fa3accef-409c-41d9-b61d-2ac6b5d8f4b6","cell_id":"8167d1ec9de942a4aab77509b756dd00","deepnote_cell_type":"code"},"source":"df_default = pd.read_csv(\"Default.csv\", index_col=0)\n\n# for now lets just drop the student varible.\ndf_default = df_default.drop(\"student\", axis=1)\ndf_default.head()","block_group":"06636ff00a894e21b9afa206bd7fed5d","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"MAqFPdjr50B-","cell_id":"8c092bbdbf6b40cabb1e39fcc39e0a2f","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 8 (CORE)\n\n1. Convert your response variable into the numerical format\n\n2. Split the data into training and test sets (**Is there anything you should try account for when splitting the data ?**) Use the test size as $10\\%$ of the whole sample\n\n3. Use the following function to get a RandomizedSearch results and sort your model results in terms of the value of \"mean_test_recall\". Comment on the obtained result in terms of accuracy and recall. \n\n**Note** that you can face with some warnings so try to examine those by searching the possible reasons on the use of `LinearSVC` below, if you prefer that function instead of `SVC`. ","block_group":"4cf6f42ce4dd464bb6b7b8a30d96eca9"},{"cell_type":"code","metadata":{"cell_id":"8f6b3f6a96a646d8b936551d60a6d079","deepnote_cell_type":"code"},"source":"from sklearn.preprocessing import LabelEncoder\n\n# Convert your response into numerical format","block_group":"a589d2eaf34b4c34a12343ade359ffc6","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"id":"Nk4JacVm6qjL","cell_id":"9ecda288937b40c98b1fad1ed17f68f4","deepnote_cell_type":"code"},"source":"C_list = []\npwr = -5\nfor i in range(6):\n    C_list.append(2**pwr)\n    pwr += 2\n\nC_list\n","block_group":"f8fdac87592e46c38e52080940d7b2fc","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"d73b57cea8de4972861c5087834288fa","deepnote_cell_type":"code"},"source":"linear_svm = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel='linear', C=1)) \n    ])\n\n\n# specify parameters and distributions to sample from\nlin_param_dist = {'svm_clf__C':loguniform(C_list[0], C_list[-1])}\n\nlin_rs = RandomizedSearchCV(linear_svm, lin_param_dist, n_iter=10, \n                            scoring = [\"accuracy\", \"f1\",\"recall\"], \n                            cv = StratifiedKFold(n_splits = 5),\n                            refit = \"recall\", \n                            random_state = 42,\n                            return_train_score = True)\n\nlin_rs.fit(X_train, y_train)","block_group":"d6bf2114bd5b4a0f9294872d8ee169bd","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"110896dbb3514c88b20b8d789220e7bd","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**","block_group":"4b550854fc444f41befac1ba8934b426"},{"cell_type":"markdown","metadata":{"id":"WdZKHECXTmgU","cell_id":"8197822e2c9f4bae8aac33fa7b90add2","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 9 (CORE)\n\nUsing the following code snippet, try different values of kernel, degree and C, what seems to produce the best model? \n\nThis is again written in terms of `SVC()` function and `GridSearchCV` for the simplicity.  \n\n(Hint: Recommended kernels are rbf, poly, and linear).\n\n**Remember that degree is a valid argument for only polynomial type of kernels. You can ignore that for the `linear` or `rbf` kernels by definition**","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"EjE0dt7Kjec3","cell_id":"37001660ef1241afa7e9a2aa1f931de6","deepnote_cell_type":"code"},"source":"# Define the pipeline steps\npipeline_steps = [\n    ('scaler', StandardScaler()), # First, scale the features\n    ('svc', SVC())                # Then, train the SVC model\n]\n\n\n# Define the parameter grid, note the 'svc__' prefix for SVC parameters\nparam_grid = { \n    'svc__kernel': ('poly', 'linear', 'rbf'), \n    'svc__C': np.linspace(0.1, 10, 10),\n    'svc__degree': [2, 3, 4]\n}\n\n# Setup the GridSearchCV with the pipeline\ncv = GridSearchCV(\n    Pipeline(pipeline_steps),\n    param_grid = param_grid, # Create a pipeline\n    cv = KFold(5, shuffle=True, random_state=42)\n)\n\n# Fit the model on the dataset\ncv.fit(X_train, y_train)\n","block_group":"3f31d43bcf704240969786c4868bcc8d","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"ae4642ee58da44098c5ca32ac8afac61","deepnote_cell_type":"code"},"source":"","block_group":"19f973b1814e474cb840ce3f36b2729a","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"c6ac79b266464bc5902870032d7bc642","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**","block_group":"8bf6ff053501442889cdbb11e676f91f"},{"cell_type":"markdown","metadata":{"id":"RaxRXbcF-5EX","cell_id":"3d3e5f62cd8f446c9bf252adbc72ada2","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 10 (EXTRA)\n\nComment out the line of code that includes the `StandardScaler` in the pipeline below. \n\n- What happens to the models predictive performance? \n\n- Try adjusting C and or kernel manually to see if you can improve the performance","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"upqmGHZ_jec4","hideCode":false,"hidePrompt":false,"cell_id":"5985b6233f1d470f949619277a003f71","deepnote_cell_type":"code"},"source":"C = ? # Use the parameter from the output of Exercise 9\nkernel = ? # Use the parameter from the output of Exercise 9\n# degree = ? # Use the parameter from the output of Exercise 9 if you get polynomial kernel\n\nm_svc = make_pipeline(\n       # StandardScaler(),\n        SVC(C=C, kernel=kernel, random_state = 42)\n    )\n\n# fitted model\nm_svc.fit(X_train,y_train)","block_group":"72e046c061184474aaca580ae8284c7e","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"4db3cf44e636451d9e6975b05d4bb1ec","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**","block_group":"8a17b0c93b53485d9da030a1f1f601c8"},{"cell_type":"markdown","metadata":{"id":"Zvf9Dact_xwz","cell_id":"e415aa438d6c4aa0b99651f495a5eb6e","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 11 (CORE)\n\nRememger that the main problem is about the data set is the imbalanced case as we observed last week. For that reason, upsampling (over-sampling) or downsampling (under-sampling) can be considered here again!\n\n- Consider adding the oversampling component using the `RandomOverSampler` within the pipeline to repeat the gridsearch in exercise 9. Focus on `rbf` kernel to reduce the computational time!\n\n- Is there any change on the best fitted model now!\n\n- Compare the `RandomOverSampler` impact in terms of confusion matrix results","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"Y1-CTLwlTaqL","cell_id":"ecb9eb9429454bca89a468c6ca5787f4","deepnote_cell_type":"code"},"source":"# Install the imblearn if necessary \n!pip install imblearn\n\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler","block_group":"e7ae0f42648748f098a046287cfdfae1","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"15e956c5913c40a79bdeb4563da9ee88","deepnote_cell_type":"code"},"source":"","block_group":"9ce4207d72e24f32a2907051e377a255","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"d6b3424a77ab42d1a60c10933ef9ef70","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**","block_group":"74a699597b7248bebd26c6f908d56bc9"},{"cell_type":"markdown","metadata":{"cell_id":"bc7318853af0481598957e44a27b99d5","deepnote_cell_type":"markdown"},"source":"---\n\n### ðŸš© Exercise 12 (EXTRA)\n\nRemember that previously we considered logistic regression on the same data set;\n\n- Fit both logistic regression and SVC model on the oversampled data set. Consider opening the argument called `probability` for the SVC model to enable probability estimates. \n\nYou can use the best set of parameters for SVC from the previous exercise output, if you completed. Otherwise, think about the use of `poly` kernel with some `C` value to implement a reasonable SVC model \n\n- Evaluate and compare the performance of the SVC and Logistic Regression models on the test set\n\n- Create ROC curve for each model on the same plot and decide which model seems better than the other","block_group":"a13ae759eb9b4eb798cdadce962cbd98"},{"cell_type":"code","metadata":{"cell_id":"f8912e8e1c3846c38fed21f669dc0e5d","deepnote_cell_type":"code"},"source":"","block_group":"5e9668a1fae14f74af63a05c80f562cd","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"619d2377fff04fd8b51c435f49641db7","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**","block_group":"ce39deffde3145f5b43053a486c23f71"},{"cell_type":"markdown","metadata":{"cell_id":"78861e4e59294fce9cb81a37518bd124","deepnote_cell_type":"markdown"},"source":"# Competing the Worksheet\n\nAt this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \nis a good time to check the reproducibility of this document by restarting the notebook's\nkernel and rerunning all cells in order.\n\nBefore generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name. If you are unable to edit the Notebook Metadata, please add a Markdown cell at the top of the notebook with your name(s).\n\nOnce that is done and you are happy with everything, you can then run the following cell \nto generate your PDF. Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. ","block_group":"c6dca49ac8c3474d850445519b14b43a"},{"cell_type":"code","metadata":{"cell_id":"03efc1901c354aa1a096226b3f2b8de1","deepnote_cell_type":"code"},"source":"!jupyter nbconvert --to pdf mlp_week07.ipynb ","block_group":"bf110138e5124c48bafbd5e1be80c974","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a2a9ec8d-a343-4210-b36b-f9db26268fc5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"a007383527e64c1e9b8fb01003652874","deepnote_execution_queue":[]}}