{"cells":[{"cell_type":"markdown","metadata":{"id":"6Og4DnJPrB4A","cell_id":"d4d18a3bd8dc46969757679a755d71dd","deepnote_cell_type":"markdown"},"source":"# Week 8 - Decision Trees\n\n### Aims\n\nBy the end of this notebook you will be able to understand \n\n>* The Basics of Decision Trees\n>* Gini Impurity\n>* Classification Trees\n>* Ensemble Models and Majority Voting\n\n\n1. [**Setup**]\n\n2. [**Basics of Decision Trees**]\n\n3. [**Gini Impurity**]\n\n4. [**Ensemble Models**]\n\n5. [**Breast Cancer Data for Binary Example**]","block_group":"00000-34f9557d-8f15-494e-8d65-74c8ae429c5c"},{"cell_type":"markdown","metadata":{"id":"AdHUSbWsvZ7h","cell_id":"c594a8c080434015a1441825f97dfdac","deepnote_cell_type":"markdown"},"source":"In this WS we will be exploring the basics of decision trees. NOTE THAT, for the simplicity we did not use any data partitioning in below for toy data examples. But for the real data set, we have the data splitting procedure as a general procedure (Breast cancer data example). ","block_group":"2af02052d7a94d0f972f3dc3ac8ba554"},{"cell_type":"markdown","metadata":{"cell_id":"29fdce0e8ca7426ebd5ae4aab7f763e3","deepnote_cell_type":"markdown"},"source":"## Reminder on Terminology\n\n- **Root node**: no incoming edge, zero, or more outgoing edges.\n\n- **Internal node**: one incoming edge, two (or more) outgoing edges.\n\n- **Leaf node**: each leaf node is assigned a class label if nodes are pure; otherwise, the class label is determined by majority vote.\n\n- **Parent and child nodes**: If a node is split, we refer to that given node as the parent node, and the resulting nodes are called child nodes.\n\n- Leaves are typically **drawn upside down**, so they are at the **bottom of the tree**. ","block_group":"c211f2257c124dea96711cefa9ca18b1"},{"cell_type":"markdown","metadata":{"cell_id":"db2421dfa3624d4c9614f4c7988dd850","deepnote_cell_type":"markdown"},"source":"## CART\n\nScikit-Learn uses an optimised version of the Classification And Regression Tree (CART) algorithm.\n\n- **Splitting Criterion**: Information gain\n- **Number of Splits**: Binary\n- **Independent Variables (Features)**: Continuous\n- **Dependent variable**: Continuous or Categorical\n- **Pruning**: Pre- & Post-pruning\n\n**Notes**\n\n\"scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.\" https://scikit-learn.org/stable/modules/tree.html\n\n## Information Gain\n\nAn algorithm starts at a tree root and then splits the data based on the feature $f$, that gives the largest information gain, $IG$\n\n- To split using information gain relies on calculating the difference between an impurity measure of a parent node, $D_p$, and the impurities of its child nodes, $D_j$; information gain being high when the sum of the impurity of the child nodes is low.\n\n- It is possible to maximise the information gain at each split using, $$IG(D_p, f) = I(D_p) - \\sum_{i=1}^{m} \\frac{N_j}{N_p} I(D_j)$$ where $I$ is the related impurity measure, $N_p$ is the total number of samples at the parent node and $N_j$ is the number of samples in the j'th child node.\n\nSome algorithms, such as Scikit-learn's implimentation of CART, reduce the potential search space by implimenting binary trees:\n\n- $IG(D_p, f) = I(D_p) - ( \\frac{N_{left}}{N_p} I(D_{left}) + \\frac{N_{right}}{N_p} I(D_{right}) )$\n\n- So to maximize the information gain, minimize the expression given in paranthesis $( \\frac{N_{left}}{N_p} I(D_{left}) + \\frac{N_{right}}{N_p} I(D_{right}) )$. That exactly matches the minimization of impurity in a different way of representation. \n\n**NOTES**\n\n- The CART algorithm is greedy - meaning it searches for the optimum split at each level. It does not check if this is the best split to improve impurity further down the tree.\n- To find the optimal tree is known as an NP-Complete problem, meaning it is intractable even for small training sets\n- Two impurity measures that are commonly used in binary decision trees are the gini impurity and entropy (log-loss) ","block_group":"32990b1e9e6c4bfb903c705636623057"},{"cell_type":"markdown","metadata":{"id":"6sVlUI4SvZ7i","cell_id":"bc8c9254bb064aae89e56e664461e6e5","deepnote_cell_type":"markdown"},"source":"# 1. General Setup <a id='setup'></a>\n\n## 1.1 Packages\n\nNow lets load in the packages you wil need for this workshop.\n","block_group":"00001-645a25eb-6010-425a-88c0-ecf0093a9edc"},{"cell_type":"code","metadata":{"id":"grVNp8GrrH0g","output_cleared":true,"cell_id":"e22b3df4ae99478faab900188a5858c1","deepnote_cell_type":"code"},"source":"# Display plots inline\n%matplotlib inline  \n\n# Data libraries\nimport pandas as pd\nimport numpy as np\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# sklearn modules list that might be useful, maybe you do not need to use all of them\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.pipeline import make_pipeline\nfrom mpl_toolkits.mplot3d import Axes3D          # 3d plots\nfrom sklearn.preprocessing import StandardScaler # scaling features\nfrom sklearn.preprocessing import LabelEncoder   # binary encoding\nfrom sklearn.pipeline import Pipeline            # combining classifier steps\nfrom sklearn.preprocessing import PolynomialFeatures # make PolynomialFeatures\nfrom sklearn.datasets import make_classification, make_moons  # make example data\nimport warnings # prevent warnings\nimport joblib # saving models\nfrom time import time\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold\nfrom scipy.stats.distributions import uniform, loguniform\nimport itertools\nfrom sklearn.model_selection import GridSearchCV, KFold\n#  from imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\n# from imblearn.metrics import classification_report_imbalanced\nimport re\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# About Tree models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\n\nfrom re import search\nimport matplotlib as mpl\n\n# For Breast cancer data \nfrom sklearn.datasets import load_breast_cancer","block_group":"00002-d0af5d8f-8894-4c5a-b754-353993666790","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"36be4cb239f4445e980bb6b29bd11852","deepnote_cell_type":"code"},"source":"# Plotting defaults MIGHT BE modified\nplt.rcParams['figure.figsize'] = (8,8)\nplt.rcParams['figure.dpi'] = 80\nplt.rcParams['lines.markersize'] = 7.5","block_group":"07edcbb07e8a49a5b114d8f8ab6ce81f","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"N8_vjOPKdqLm","cell_id":"5cdcb70e236d4c66970613f982b07abc","deepnote_cell_type":"markdown"},"source":"##  1.2 Palmer Data Set","block_group":"51fc0ec3a3754c548e2a8d7524677b47"},{"cell_type":"code","metadata":{"cell_id":"e5c7a31fff0f4a4ca2920d110de0a2d0","deepnote_cell_type":"code"},"source":"# Install the data set\n!pip install palmerpenguins\n\n# Palmer Penguins Data\nfrom palmerpenguins import load_penguins","block_group":"7caeae41be324198a29e1febd01b813c","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"yz3bjxcbvZ7r","cell_id":"2290baf8a19b4b3988b02e9f4626887c","deepnote_cell_type":"markdown"},"source":"# 2. PalmerPenguins Data <a id='DecTree'></a>\n\nThe \"palmer penguins\" dataset contains data for 344 penguins from 3 different species and from 3 islands in the Palmer Archipelago, Antarctica.","block_group":"00003-90709695-8746-4669-9199-fd144a6ec872"},{"cell_type":"code","metadata":{"cell_id":"a711a9e809924528a9e5e19dfb44f12a","deepnote_cell_type":"code"},"source":"penguins = sns.load_dataset(\"penguins\")\ndisplay(penguins.head())","block_group":"c95430eceecc42e6aff7aea81e20b3b3","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"id":"-mG-rvtrvZ7u","output_cleared":true,"cell_id":"a348bc6b0b364bfc8f1cb200bda65b40","deepnote_cell_type":"code"},"source":"Since there are some missing values, they should be removed first","block_group":"00006-4d9907ea-c7c7-4c16-aee7-5e12c3e00cc1","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"43feff8eef854b5f9422a49f6d3e2865","deepnote_cell_type":"code"},"source":"# dropna values\npenguins_rm = penguins.dropna()\ndisplay(penguins_rm.head())","block_group":"9e2347b1d5154c59ad2d9e80a096a184","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"d2a889ffdf2045fd8a9fba66c948f01d","deepnote_cell_type":"code"},"source":"penguins_rm.info()","block_group":"444d82673e8d4a8c98d6f5a7caa01c29","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"eb1e70301443422d90c6969ca0e85b1d","deepnote_cell_type":"code"},"source":"# Look at the summary of continuos variable\npenguins_rm.describe().round(2)","block_group":"1306b20dbf60471987083cda8fa64f21","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"79af7f58ec8f47078b0d78c34f5243a0","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 1 (CORE)\n\nBased on the given plot below, \n\n- Which species is easy to separate by looking at the plot ?\n- Which set of features might be the good choices to use for a classification problem ?\n","block_group":"b3f5be78da564bdcac2b5aac31408c32"},{"cell_type":"code","metadata":{"cell_id":"6b1413ae053b49de83a7463472b2855c","deepnote_cell_type":"code"},"source":"# Plotting the data set\nsns.pairplot(data = penguins_rm, hue = \"species\")","block_group":"21751339cb1f4df1b6e951e72514d934","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"90f36fffff9a4db381afe04a21dc359c","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 2  (CORE)\n\nConsider the following decision tree fit on the penguins data, based on the selected predictors and try to answer the followings;\n\n- What is the meaning of **max_depth** in the given code snippet ?\n\n- Identify the predictor selected in the top/root node\n\n- State the **type of nodes** illustrated below\n\n- What is the meaning of the numbers inside of Value for each node ?","block_group":"905d7ff6968047baa0fdc2bffe34e8a4"},{"cell_type":"code","metadata":{"cell_id":"8fe7b8539a514e2c97a4460378a763a4","deepnote_cell_type":"code"},"source":"# Some selection on data\nX = penguins_rm.drop([\"island\", \"sex\", \"species\", \"body_mass_g\"], axis=1)\ny = penguins_rm.species ","block_group":"482e0d56de8c47309810049d72549b01","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"157b1cc75ea6441898e2d9d47775ed04","deepnote_cell_type":"code"},"source":"# Fitting DecisionTreeClassifier\ntree_clf = DecisionTreeClassifier(max_depth = 2)\ntree_clf.fit(X, y)\n\n# You can visualize the trained Decision Tree\nfrom sklearn.tree import export_graphviz\n\ntree_data = export_graphviz(\n tree_clf,\n rounded = True, filled = True\n )\n\n# For plotting simply\nimport graphviz \ngraph = graphviz.Source(tree_data) \ngraph","block_group":"340be659865940d3820327dd97db46d5","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"4xhjuXdcvZ7v","cell_id":"7cc2c58adb75419dac26bb4f8a35107e","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your comments about the answer here !!!**","block_group":"c848f397a53345dcb98325f24574e0f2"},{"cell_type":"markdown","metadata":{"cell_id":"8d70f69a92f043bda3e082a01f1bbdeb","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 3  (CORE) <a id='gini'></a>\n\nRelated to the information gain calculation, we have Gini or entropy impurity\n\n- Work out on the calculations of the related Gini index values in the leaf nodes for the given output\n\n- Compare your results with your own calculations. Do they match exactly or not ?\n\n- Update the above code snippet by changing the criterion from **Gini** to **Entropy** in the main function and get the similar output- Update the above code snippet by changing the criterion from **Gini** to **Entropy** in the main function and get the similar output\n\n**RECALL**\n\n- **Gini Index**: It is a measure of total variance across the $K$ classes, defined as follows:\n\n$G = \\sum_{k=1}^{K} \\widehat{p}_{mk} (1 - \\widehat{p}_{mk}) = \n1 - \\sum_{k=1}^{K} \\widehat{p}_{mk}^2$ \n\nwhere $\\widehat{p}_{mk}$ is the proportion of training observations in the $m$'th region (belong to $mth$ node) coming from the $k$'th class.\n\n- **Entropy**: It is an alternative to Gini Index and defined as; \n\n$D = - \\sum_{k=1}^{K} \\widehat{p}_{mk} \\log \\widehat{p}_{mk}$\nwhere the range of $\\widehat{p}_{mk}$ guarantees that $0 \\leq - \\widehat{p}_{mk} \\log \\widehat{p}_{mk}$.","block_group":"259c6d6443a6430485605fea57e393da"},{"cell_type":"code","metadata":{"cell_id":"ec8e73184710436290f212a0e9280127","deepnote_cell_type":"code"},"source":"","block_group":"9fabaf2cd6674800ab353830bd396209","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"da9c10a53771446297de36bf6b3852a5","deepnote_cell_type":"code"},"source":"🏁 Now, is a good point to switch driver and navigator","block_group":"fff975f75ef749f8908b160ddd5e94e3","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"73770b54d0c14078ac572211ee0a5b76","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 4  (CORE)\n\nWhat is the accuracy of the fitted decision tree above over the 5-fold CV? \n\n<details><summary><b><u>Hint</b></u></summary>\n    \n- You can use simply the function of  `cross_val_score`\n    \n</details>\n\n","block_group":"dbae406b816848e08681337587ce3388"},{"cell_type":"code","metadata":{"cell_id":"549f1d3fbea54f87b1f5bfb10529961b","deepnote_cell_type":"code"},"source":"from sklearn.model_selection import cross_val_score","block_group":"9bc47d892c014d529b73c0ffd8fa68c6","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"4xhjuXdcvZ7v","cell_id":"fb05b4c73e7146acaef111727a7d8239","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your comments about the answer here !!!**","block_group":"e1411ad4f5614d4d8aa50d259a6b6565"},{"cell_type":"markdown","metadata":{"cell_id":"3395a18b9bcd4cce80162a2cb0599985","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 5 (CORE)\n\nPlay around some of the input arguments, such as\n\n- `max_depth`\n\n- `min_samples_split`\n\n- `min_samples_leaf`\n\n- `max_leaf_nodes`\n\n- `min_impurity_decrease`\n\nby changing their values\n\n- Rerun different models with such selections and compare / observe their performances briefly","block_group":"19d8351064c4462a948c44252eccaae3"},{"cell_type":"markdown","metadata":{"id":"_YZt5su8C7WZ","cell_id":"3bdfa8fe790f4453b212936b0b18b276","deepnote_cell_type":"markdown"},"source":"# 3. Ensemble I: Bagging  <a id='majVot'></a> \n\nA bagging classifier is an ensemble of base classifiers, each fit on random subsets of a dataset. Their predictions are then pooled or aggregated to form a final prediction. To apply bagging to decision trees, we simply construct\n\n(i) decision trees using bootstrapped training sets,\n(ii) take a majority vote of the resulting predictions.\n\n- A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.\n\n- Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n\n- Specifically, bagging is when sampling is produced with replacement, and without replacement being called pasting\n\n- Pasting is designed to use smaller sample sizes than the training dataset in cases where the training dataset does not fit into memory\n\n- Both bagging and pasting allow training to be sampled several times across multiple predictors, with bagging only allowing several samples for the same predictor\n\n- Averaging methods generally work best when the predictors are as independent as possible, so one way of achiving this is to get diverse classifiers\n\n- Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.\n\nFor some details on sklearn, please see that webpage \nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n\n","block_group":"9f70928844354aab8d73c9308696d161"},{"cell_type":"markdown","metadata":{"id":"mvrOq4afvZ7z","cell_id":"6df88d1cf9bb4274943b6def8091688d","deepnote_cell_type":"markdown"},"source":"\n### 🚩 Exercise 6  (CORE)\n\nExecute the following code and try to answer the followings:\n\n- What is the predicted class labels based on given Bagging model, using the same predictors that we used in Exercise 2\n\n- How do you predict a new data having $(bill\\_length\\_mm,bill\\_depth\\_mm,flipper\\_length\\_mm)$ = (15.6, 34.7, 185.4) ? ","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"i2qXwFXvQbBq","cell_id":"75d2c1e916e24cd09e738b717cc2f822","deepnote_cell_type":"code"},"source":"from sklearn.ensemble import BaggingClassifier\nclf = BaggingClassifier(n_estimators=10, random_state=42).fit(X, y)","block_group":"9afe824b43c34575bb2a29809eb4ac32","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"adb2eba9a59b42449fe2f1a8cc3deacb","deepnote_cell_type":"code"},"source":"","block_group":"ec31cc344ac844c187e08d170ce5ab1a","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"ZecdeX9yQdVh","cell_id":"530989f78c734601b59d0a482b7f64ed","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 7  (CORE)\n\nBased on the above fitted model,\n\n1. Calculate the out-of-bag score\n\n2. Examine the impact of `n_estimators` by using different values in $(1, 100, 1000)$\n\n3. Compare all the out-of-bag scores of all fitted models under different `n_estimators` values","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"3Mxs7ORhvZ71","output_cleared":true,"cell_id":"856a720bc5494530a7c797c0b57d0cd0","deepnote_cell_type":"code"},"source":"","block_group":"00023-04722d32-f420-4104-b25b-37656a71df76","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"TLaXXgVqGQrd","cell_id":"d97a16deafd841d48912b18ef262ac6d","deepnote_cell_type":"markdown"},"source":"## 4. Ensemble II: RandomForest\n\nRandom forests are among the most widely used machine learning algorithm. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Random forests are essentally bagged tree classifiers, but decorrelate the trees by using a random sample of features each time a split in a tree is considered. The random forest algorithm can therefore be summarized in four steps;\n\n- Draw a random bootstrap sample of size $n$\n\n- Grow a decision tree from the bootstrap sample. At each node:\n    * Randomly select $d$ features without replacement (typically the square root of the total number of predictors).\n    * Split the node using the feature that provides the best split according to the objective function.\n\n- Repeat the steps above $k$ times.\n\n- Aggregate the prediction by each tree to assign the class label by majority vote.\n\n\n**NOTES** \n\n- The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.\n\n- Random forests fit decision trees on different bootstrap samples, and for each decision tree, select a random subset of features at each node to decide upon the optimal split. The feature subset to consider at each node is a hyperparameter that we can tune\n\n- Instead of using majority vote, in Sklearn the `RandomForestClassifier` averages the probabilistic prediction.\n\n- Notice that if a random forest is built using all features, then this is simply bagging.\n\n- you can also bootstrap features in the `BaggingClassifier` using `bootstrap_features=True`\n\n- By not allowing the model to use the majority of the available predictors, we ensure the bagged trees look different from each other.\n\n- If there is a particularly strong set of predictors in the data, then without randomly selecting features, the bagged trees will look quite similar to each other and predictions will be highly correlated. Averaging highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities\n\n### Related Hyperparameters\n\nImportant parameters to adjust are;\n\n- `n_estimators`: Larger is generally better as averaging more trees will typically yield a more robust ensemble by reducing overfitting.\n- `max_features`: Determines how random each tree is so smaller number of features at each split reduces overfitting.\n- `max_samples`: Sample size of the bootstrap sample, also reduces overfitting. pre-pruning options (`max_depth`, `max_leaf_nodes`): more important for single trees, but can improve performance, reduce space, and time requirements.\n\n**NOTES**\n\n- Typically you want to use as many estimators as you have time and memory for training.\n\n- A good rule of thumb for `max_features` default values are `max_features=sqrt(n_features)` for classification and `max_features=n_features` for regression\n\n- \"Segal (2004) showed that if your data has many noisy predictors and higher `[max_features]` values are performing best, then performance may improve by increasing node size (i.e., decreasing tree depth and complexity). Moreover, if computation time is a concern then you can often decrease run time substantially by increasing the node size and have only marginal impacts to your error estimate...\"\n\n- Small bootstrap samples do tend to produce worse models\n\nFor a more thorough discussion of forest hyperparameters, see: Probst, Philipp, Bernd Bischl, and Anne-Laure Boulesteix. 2018. “Tunability: Importance of Hyperparameters of Machine Learning Algorithms.” arXiv Preprint arXiv:1802.09596.","block_group":"77b3f94403994b6c827b2d750983bd6d"},{"cell_type":"markdown","metadata":{"cell_id":"5a2bce500163447faf0047f49dcb9a90","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 8 (CORE)\n\nConsider the above problem under randomforest approach simply using the `RandomForestClassifier()` function adjusting the **max_depth** value","block_group":"176200a47d5941a893e43d4432277f1f"},{"cell_type":"code","metadata":{"cell_id":"469fa9f1f9e0420da636737e784ea691","deepnote_cell_type":"code"},"source":"from sklearn.ensemble import RandomForestClassifier","block_group":"56e0995bb718402e8351536e1b60705d","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"z4NLHMUEdKxP","cell_id":"d2462ab7c5a049db9c5660003401a864","deepnote_cell_type":"markdown"},"source":"---\n \n### 🚩 Exercise 9 (CORE)\n\nSet up a function for experimenting with different n_estimators and max_depth. For this purpose, consider, \n\n1. **n_estimators** in $[1,10,100, 1000]$\n2. **max_depth** in $[2, 3, 4]$\n\nFind the final out-of-bag score based on the optimal one based on your comparison on the accuracy","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"9vTpIe932BFz","cell_id":"af732321ae09483297762e11ab0a8c55","deepnote_cell_type":"code"},"source":"","block_group":"29337b2afe8c48539bdc43596ef55de7","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"a7ca9ef4eaef4c3caa265a149aa395e9","deepnote_cell_type":"markdown"},"source":"🏁 Now, is a good point to switch driver and navigator","block_group":"2dc1ae3163d54cf4a2c1e8cc91681d33"},{"cell_type":"markdown","metadata":{"cell_id":"7dccad5e4cd64497ac8991edcd3280fa","deepnote_cell_type":"markdown"},"source":"# 5 Voting Classifier\n\nMajority voting can be done by simply selecting the class label that has been predicted by the majority of the classifiers (more than 50% of votes). Majority vote refers to binary class decisions but can be generalized to a multi-class setting using plurality voting\n\nIn majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. Simply\n\nE.g., if the prediction for a given sample is\n\n- classifier 1 -> class 1\n\n- classifier 2 -> class 1\n\n- classifier 3 -> class 2\n\nthe VotingClassifier (with voting='hard' by default) would classify the sample as “class 1” based on the majority class label.\n\n**NOTES**\n\n- Scikit-learn uses the `predict_proba` method to compute class probabilities.\n\n- \"In decision trees, the probabilities are calculated from a frequency vector that is created for each node at training time. The vector collects the frequency values of each class label computed from the class label distribution at that node. Then, the frequencies are normalized so that they sum up to 1... Although the normalised probabilities returned by both the decision tree and k-nearest neighbors classifier may look similar to the probabilities obtained from a logistic regression model, we have to be aware these are actually not derrived from probability mass functions.\"\n\nFor further details, please see the documentation from here: https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier","block_group":"9fc1184d74bf41a693514be57ccd9ccd"},{"cell_type":"markdown","metadata":{"id":"wYXQZYkTMy7M","cell_id":"aabca967d2734c47bf7ea6c0d15987b3","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 10  (EXTRA)\n\n- Complete the following code snippet with your RandomForest and Bagging model selection above\n\n- Look at the accuracy score of the each model and voting classifier one, is there any improvement with voting classifier or not ?\n","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"cell_id":"25e3fa8db9a7480699d9bbb4f538e672","deepnote_cell_type":"code"},"source":"# As an example for different models \nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Your bagging model\nbag_clf = ____________\n\n# Your random forest model\nrnd_clf = _____________\n\n# Default SVC model\nsvm_clf = SVC()\n\nvoting_clf = VotingClassifier(\n estimators=[('bag', bag_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n voting='hard')\n\nvoting_clf.fit(X, y)","block_group":"10517e6294c84d79a82de6132d4d9052","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"YwCfjeYB4tVM","cell_id":"4e53ba37da554a90b32e8d70a0b93704","deepnote_cell_type":"markdown"},"source":"# 6 Breast Cancer Data Case <a id='cancer'></a>\n\nLoad and return the breast cancer wisconsin dataset (classification). The breast cancer dataset is a classic and very easy binary classification dataset since we have two cancer types, WDBC-Malignant and WDBC-Benign.\n\nYou can find other details about the data set from here: https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset\n\n\n","block_group":"00003-90709695-8746-4669-9199-fd144a6ec872"},{"cell_type":"code","metadata":{"id":"SlbaLT675J8g","colab":{"height":206,"base_uri":"https://localhost:8080/"},"outputId":"fa3accef-409c-41d9-b61d-2ac6b5d8f4b6","cell_id":"8a736ec57c99414e85263f8b4a499d26","deepnote_cell_type":"code"},"source":"from sklearn.datasets import load_breast_cancer\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)","block_group":"09ba95d1c7cf475fbdd747b997828e0b","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"MAqFPdjr50B-","cell_id":"5aa9ffb852d644a4930c9fc55f29a6e2","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 11 (CORE)\n\n1. Split the data into training and test sets (**Is there anything you should try account for when splitting the data ?**) Use the test size as $30\\%$ of the whole sample\n\n2. Consider a `DecisionTree classifier` on the training data set using all features\n\n3. Compute the pruning path during `Minimal Cost-Complexity Pruning`. Examine and explain your findings\n\nNote that \n\n- Minimal cost complexity pruning recursively finds the node with the “weakest link”. The weakest link is characterized by an effective alpha, where the nodes with the smallest effective alpha are pruned first. \n\n- To get an idea of what values of ccp_alpha could be appropriate, scikit-learn provides DecisionTreeClassifier.cost_complexity_pruning_path that returns the effective alphas and the corresponding total leaf impurities at each step of the pruning process. \n\n- As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves.\n\n<details><summary><b><u>Hint</b></u></summary>\n    \n- You can use simply the function of  `cost_complexity_pruning_path`\n    \n- See further details from here: https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning\n    \n</details> ","block_group":"5afd8d91debe40d486a73ac8c8e47e8c"},{"cell_type":"code","metadata":{"id":"Nk4JacVm6qjL","cell_id":"0d03bc16ae4c43ae89df7ec606bf4c27","deepnote_cell_type":"code"},"source":"","block_group":"3d745480ea2f4654925f4f28b01349e9","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"9dc1dc122e5941ebb1360dd364c8bd01","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 12 (EXTRA)\n\nInside of pruning path during (in the output of `cost_complexity_pruning_path`), we have different applicable alpha values and corresponding impurity in the leaf nodes introduced. This contains two Numpy Arrays of alpha and impurities in general. Using the obtained values, \n\n- Simply plot that relationship based on alpha and corresponding impurity values\n\n- Find an optimal value of alpha by using the model accuracy (looking at the test data performance mainly)\n\n- With this selected alpha value, consider RandomForestClassifier rather than the DecisionTreeClassifier to create a new ensemble model\n\n- Calculate the variable importance by using `feature_importances_` and visualize. You can benefit from the example given here: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html","block_group":"37d366fdcbb646b991181eb24f03fddb"},{"cell_type":"code","metadata":{"cell_id":"d95aba10118e4addbdb280d4672d0c7a","deepnote_cell_type":"code"},"source":"","block_group":"9c4a185f89c84f65becf408ad090a39a","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"f37eaaecbd304a5e9c060030f21a1856","deepnote_cell_type":"markdown"},"source":"# Competing the Worksheet\n\nAt this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \nis a good time to check the reproducibility of this document by restarting the notebook's\nkernel and rerunning all cells in order.\n\nBefore generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name. If you are unable to edit the Notebook Metadata, please add a Markdown cell at the top of the notebook with your name(s).\n\nOnce that is done and you are happy with everything, you can then run the following cell \nto generate your PDF. Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. ","block_group":"5fcb2c3f68e646549ccd451bf84e3a1c"},{"cell_type":"code","metadata":{"cell_id":"785e0704363a41aaa79472f856f78cf0","deepnote_cell_type":"code"},"source":"!jupyter nbconvert --to pdf mlp_week08.ipynb ","block_group":"332ec78a3ccf4e02ac5038dea07ae9dc","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a2a9ec8d-a343-4210-b36b-f9db26268fc5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"3a35a7d22cf94852bf60e34a5e082068","deepnote_execution_queue":[]}}