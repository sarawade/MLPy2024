{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNnuZONqYsb4"
   },
   "source": [
    "# Week 3: Clustering\n",
    "\n",
    "\n",
    "In this workshop, we will work through a set of problems clustering, another cannonical form of unsupervised learning. Clustering is an important tool that is used to discover homogeneous groups of data points within a heterogeneous population. It can be the main goal in some problems, while in others it may be used in EDA to understand the main types of behavior in the data or in feature engineering.   \n",
    "\n",
    "We will start by generating some artificial data, and then we will utilize clustering algorithms described in lectures and explore the impact of feature engineering on the solution. We will then attempt to find clusters in a gene expression dataset. \n",
    "\n",
    "As usual, the worksheets will be completed in teams of 2-3, using **pair programming**, and we have provided cues to switch roles between driver and navigator. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    ">- Look for the üèÅ as cue to switch roles between driver and navigator.\n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given. \n",
    "\n",
    "As you work through the problems it will help to refer to your lecture notes (navigator). The exercises here are designed to reinforce the topics covered this week. Please discuss with the tutors if you get stuck, even early on! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. [Problem Definition and Setup: Simulated Example](#setup1)\n",
    "2. [K-means: Simulated Example](#kmeans1)\n",
    "3. [Hierarchical Clustering: Simulated Example](#hc)\n",
    "4. [Gene Expression Data](#genedata)\n",
    "5. [Hierarchical Clustering: Gene Expression Data](#hc_genedata)\n",
    "6. [K-means Clustering: Gene Expression Data](#kmeans_genedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAdywZYoZa5T"
   },
   "source": [
    "# Problem Definition and Setup: Simulated Example <a id='setup1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "First, lets load in some packages to get us started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "am3xGa8BYohT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWAWprSdZ11V"
   },
   "source": [
    "## Data: Simulated Example\n",
    "\n",
    "We will begin with a simple simulated example in which there are truly three clusters. We assume that there are $D=2$ features and within each cluster, the data points are generated from a spherical normal distribution $N(\\mathbf{m}_k, \\sigma^2_k \\mathbf{I})$ for clusters $k=1,2,3$, where both the mean $\\mathbf{m}_k$ and variance $\\sigma^2_k$ are different across clusters. Specifically, we assume that: \n",
    "\n",
    "* Cluster 1: contains $|C_1|=500$ points with mean vector $\\mathbf{m}_1 = \\begin{pmatrix} 0 \\\\ 4 \\end{pmatrix}$ with standard deviation $\\sigma_1 = 2$.\n",
    "* Cluster 2: contains $|C_2|=250$ points with mean vector $\\mathbf{m}_2 = \\begin{pmatrix} 0 \\\\ -4 \\end{pmatrix}$ with standard deviation $\\sigma_2 = 1$.\n",
    "* Cluster 3: contains $|C_3|=100$ points with mean vector $\\mathbf{m}_3 = \\begin{pmatrix} -4 \\\\ 0 \\end{pmatrix}$ with standard deviation $\\sigma_3 = 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 1 (CORE)\n",
    "\n",
    "Generate the dataset described above and store it in array called `X`.\n",
    "\n",
    "Visualise your data and color by the true cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzWG5w4SZv8q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering: Simulated Example <a id='kmeans1'></a>\n",
    "\n",
    "To perform K-means clustering, we will use `KMeans()` in `sklearn.cluster`. Documentation is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and for an overview of clustering methods available in `sklearn`, see [link](https://scikit-learn.org/stable/modules/clustering.html). There are different inputs we can specify when calling `KMeans()` such as:\n",
    "\n",
    "- `n_clusters`: the number of clusters. \n",
    "- `init`: which specifies the intialization of the centroids, e.g. can be set to `k-means++` for K-means++ initialization or `random` for random initialization.\n",
    "- `n_init`: which specifies the number of times the algorithm is run with different random initializations\n",
    "- `random_state`: this can bet set to a fixed number to make results reproducible.\n",
    "\n",
    "We can then use the `.fit()` method of `KMeans` to run the K-means algorithm on our data.\n",
    "\n",
    "After fitting, some of the relevant attributes of interest include:\n",
    "\n",
    "- `labels_`: cluster assignments of the data points.\n",
    "- `cluster_centers_`: mean corresponding to each cluster, stored in a matrix of size: number of clusters $K$ times number features $D$.\n",
    "- `inertia_`: the total within-cluster variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 2 (CORE)\n",
    "\n",
    "Let's start by exploring how the clustering changes across the K-means iterations. To do, set:\n",
    "\n",
    "- number of clusters to 3\n",
    "- initialization to random\n",
    "- number of times the algorithm is run to 1\n",
    "- fix the random seed to a number of your choice (e.g. 2)\n",
    "\n",
    "\n",
    "a) Now, fit the K-means algorithms with different values of the maximum number of iterations fixed to 1,2,3, and the default value of 300. \n",
    "\n",
    "b) Plot the clustering solution for the four different cases and comment on how it changes. \n",
    "\n",
    "c) How many iterations are needed for the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 3 (CORE)\n",
    "\n",
    "Next, compare the random intialization with K-means++ (in this case fix the number of different initializations to 10). Plot both clustering solutions. Which requires fewer iterations? and which provides a lower within-cluster variation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 4 (CORE)\n",
    "\n",
    "Find the clustering solution using a different number of initializations equal to 1, 2, 5, 10, and 20. Visualize and comment on the results. Try changing the random state; how does that change your conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 5 (CORE)\n",
    "\n",
    "Since we simulated the data, we know the true number of clusters. However, in practice this number is rarely known. Find the K-means solution with different choices of $K$ and plot the within-cluster variation as a function of $K$. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 6 (CORE)\n",
    "\n",
    "Now standardize the data and re-run the K-means algorithm. Qualitatively, how has standardising the data impacted performance? Can you argue why you observe what you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering: Simulated Example <a id='hc'></a>\n",
    "\n",
    "To perform hierarchical clustering, we will use the [`linkage()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) function from `scipy.cluster.hierarchy`. The inputs to specify include\n",
    "\n",
    "-  the data. \n",
    "- `metric`: specifies the dissimarlity between data points. Defaults to the Euclidean distance.\n",
    "- `method`: specifies the type of linkage, e.g. complete, single, or average.\n",
    "\n",
    "Then, we can use [`dendrogram()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html) from `scipy.cluster.hierarchy` to plot the dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 7 (CORE)\n",
    "\n",
    "a) Use hierarchical clustering with complete linkage to cluster the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either the name of the returned object from hierarchy.linkage should be hc_comp, or change the name in part b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot the dendogram by running the code below. Try changing the 'color_threshold' to a number (e.g. 11) to color the branches of the tree below the threshold with different colors, thus, identifying the clusters if the tree were to be cut at that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to replace hc_comp with your chosen name for the returned object from hierarchy.linkage in part a\n",
    "cargs = {'color_threshold': -np.inf,'above_threshold_color':'black'}\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "hierarchy.dendrogram(hc_comp, ax=ax, **cargs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Now, use the function `cut_tree()` from `scipy.cluster.hierarchy` to determine the cluster labels associated with a given cut of the dendrogram. You can either specify the number of clusters via `n_clusters` or the height/threshold at which to cut via `height`. Plot the data colored by cluster membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 8 (CORE)\n",
    "\n",
    "Now try changing the linkage to single and average. Does this affect on the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG5WyUN2GbiB"
   },
   "source": [
    "# Gene Expression Data <a id='genedata'></a>\n",
    "\n",
    "Now, we will consider a more complex real dataset with a larger feature space. \n",
    "\n",
    "The dataset is the **NCI cancer microarray dataset** discussed in both *Introduction to Statistical Learning* and  *Elements of Statistical Learning*. The dataset consists of $D=6830$ gene expression measurements for each of $N=64$ cancer cell lines. The aim is to determine whether there are groups among the cell lines based on their gene expressions. This is an example of a high-dimensional dataset with $D$ much larger than $N$, which makes visualization difficult. The $N=64$ cancer cell lines have been obtained from samples of cancerous tisses, corresponding to 14 different types of cancer. However, our focus remains unsupervised learning and we will use the cancer labels only to plot. \n",
    "\n",
    "We first need to read in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leJH2StnFPaI"
   },
   "outputs": [],
   "source": [
    "#Fetch the data and cancer labels\n",
    "url_data = 'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv'\n",
    "url_labels = 'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label.txt'\n",
    "\n",
    "X = pd.read_csv(url_data)\n",
    "y = pd.read_csv(url_labels, header=None)\n",
    "\n",
    "# clean data and follow convention in the notes that features are columns:\n",
    "X = X.drop(labels='Unnamed: 0', axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H97lGxLsOa9v"
   },
   "source": [
    "Let's visualise the data with a contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "Tb1J3rLMNula",
    "outputId": "0eb88c8e-0415-4be5-f646-1b677105071c"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,5))\n",
    "ax = fig.add_subplot(111)\n",
    "contours = ax.contourf(X, cmap='inferno')#, vmax=4, vmin=-4)\n",
    "cbar = plt.colorbar(contours)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(\"Gene expression\", fontsize=22)\n",
    "ax.set_ylabel(\"Tissue sample\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mozq7WKja2gW"
   },
   "source": [
    "We now convert our pandas dataframe into a numpy array and create integer labels for cancer type (for plotting purposes)\n",
    "\n",
    "If you visualise the labels, you will notice there are lots of inconsistencies with white space etc. Run the following code to clean the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clean = np.asarray(y).flatten()\n",
    "for j in range(y_clean.size):\n",
    "    y_clean[j] = y_clean[j].strip()\n",
    "np.unique(y_clean, return_counts = True)\n",
    "cancer_types = list(np.unique(y_clean))\n",
    "cancer_groups = np.array([cancer_types.index(lab) for lab in y_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13qbb2jqbvhT"
   },
   "source": [
    "### üö© Exercise 9 (EXTRA)\n",
    "\n",
    "Perform a PCA of $\\mathbf X$ to visualize the data. Plot the first few principal component scores and color by cancer type. Do cell lines within the same cancer types seems to have similar scores? Make a scree plot of the proportion of variance explained. How many components does this suggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEdWUy88bYMi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering: Gene Expression Data <a id='hc_genedata'></a>\n",
    "\n",
    "Now, let's perform hierarchical clustering on the gene expression data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 10 (CORE)\n",
    "\n",
    "a) Plot the dendrogram with complete, single, and average linkage. Does the choice of linkage affect the results? Which linkage would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Select a linkage and a number of clusters (by examining the dendrogram and jumps in the heights of the clusters merged). Plot the dendogram and color the branches to identify the clusters. Use the option `labels = np.asarray(y_clean), leaf_font_size=10` in `hierarchy.dendrogram` to add the cancer types as labels for each data point. Do you observe any patterns between the clusters and cancer types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering: Gene Expression Data <a id='kmeans_genedata'></a>\n",
    "\n",
    "Now, let's perform k-means clustering on the gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 11 (CORE)\n",
    "\n",
    "Perform K-means clustering with the same number of clusters that you selected for hierarchical clustering. Are the results similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 13 (EXTRA)\n",
    "\n",
    "Plot the two clustering solutions along with a plot of the data colored by the cancer types in the space spanned by the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So3kpRjk2-yx"
   },
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name.\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week03.ipynb "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Student 1"
   },
   {
    "name": "Student 2"
   }
  ],
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "title": "MLPy Workshop 3"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
