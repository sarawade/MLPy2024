{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-34f9557d-8f15-494e-8d65-74c8ae429c5c",
    "deepnote_cell_type": "markdown",
    "id": "gRJp4fGPvZ7c"
   },
   "source": [
    "# Week 4 - Regression and Model Evaluation\n",
    "\n",
    "## Aims\n",
    "\n",
    "By the end of this notebook you will be able to \n",
    "\n",
    ">* fit a linear regression\n",
    ">* understand the basics of polynomial regression\n",
    ">* understand how to evaluate and compare models and select tuning parameters with training, validation, and testing.\n",
    "\n",
    "1. [Problem Definition and Setup](#setup)\n",
    "\n",
    "2. [Exploratory Data Analysis](#EDA)\n",
    "\n",
    "3. [Least Squares Estimation](#RBH)\n",
    "\n",
    "4. [Regression using scikit-Learn](#RSKL)\n",
    "\n",
    "5. [Polynomial Regression](#polyreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdHUSbWsvZ7h"
   },
   "source": [
    "During workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. From this week onwards, the worksheets will no longer contain cues to switch roles between driver and navigator; this should occur approximately every 15 minutes and should be more natural after the first weeks. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-645a25eb-6010-425a-88c0-ecf0093a9edc",
    "deepnote_cell_type": "markdown",
    "id": "6sVlUI4SvZ7i"
   },
   "source": [
    "---\n",
    "\n",
    "# Problem Definition and Setup <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEIja_2pvZ7n"
   },
   "source": [
    "## Packages\n",
    "\n",
    "First, let's load the packages you wil need for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-d0af5d8f-8894-4c5a-b754-353993666790",
    "deepnote_cell_type": "code",
    "id": "KBfh_AXdvZ7o",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn modules and some other will be added later\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults for all Figures below\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC7t8FfQvZ7p"
   },
   "source": [
    "## User Defined Helper Functions\n",
    "\n",
    "Below are two helper functions we will be using in this workshop. You can create your own if you think it is useful or simply use already available functions within `sklearn`.  \n",
    "\n",
    "- `get_coefs()`: Simple function that extracts both the intercept and coefficients from the model in the pipeline and then concatenates them.\n",
    "- `model_fit()`: Returns the mean squared error, root mean squared error and R^2 value of a fitted model based \n",
    "    on provided X and y values with plotting as add-on.\n",
    "\n",
    "Feel free to also modify the functions based on your needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biPA_OxTvZ7p"
   },
   "outputs": [],
   "source": [
    "def get_coefs(m):\n",
    "    \"\"\"Returns the model coefficients from a Scikit-learn model object as an array,\n",
    "    includes the intercept if available.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If pipeline, use the last step as the model\n",
    "    if (isinstance(m, sklearn.pipeline.Pipeline)):\n",
    "        m = m.steps[-1][1]\n",
    "    \n",
    "    \n",
    "    if m.intercept_ is None:\n",
    "        return m.coef_\n",
    "    \n",
    "    return np.concatenate([[m.intercept_], m.coef_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(m, X, y, plot = False):\n",
    "    \"\"\"Returns the mean squared error, root mean squared error and R^2 value of a fitted model based \n",
    "    on provided X and y values.\n",
    "    \n",
    "    Args:\n",
    "        m: sklearn model object\n",
    "        X: model matrix to use for prediction\n",
    "        y: outcome vector to use to calculating rmse and residuals\n",
    "        plot: boolean value, should fit plots be shown \n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = m.predict(X)\n",
    "    MSE = mean_squared_error(y, y_hat)\n",
    "    RMSE = np.sqrt(mean_squared_error(y, y_hat))\n",
    "    Rsqr = r2_score(y, y_hat)\n",
    "    \n",
    "    Metrics = (round(MSE, 4), round(RMSE, 4), round(Rsqr, 4))\n",
    "    \n",
    "    res = pd.DataFrame(\n",
    "        data = {'y': y, 'y_hat': y_hat, 'resid': y - y_hat}\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        sns.lineplot(x='y', y='y_hat', color=\"grey\", data =  pd.DataFrame(data={'y': [min(y),max(y)], 'y_hat': [min(y),max(y)]}))\n",
    "        sns.scatterplot(x='y', y='y_hat', data=res).set_title(\"Actual vs Fitted plot\")\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        sns.scatterplot(x='y_hat', y='resid', data=res).set_title(\"Fitted vs Residual plot\")\n",
    "        plt.hlines(y=0, xmin=np.min(y), xmax=np.max(y), linestyles='dashed', alpha=0.3, colors=\"black\")\n",
    "        \n",
    "        plt.subplots_adjust(left=0.0)\n",
    "        \n",
    "        plt.suptitle(\"Model (MSE, RMSE, Rsqr) = \" + str(Metrics), fontsize=14)\n",
    "        plt.show()\n",
    "    \n",
    "    return MSE, RMSE, Rsqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-90709695-8746-4669-9199-fd144a6ec872",
    "deepnote_cell_type": "markdown",
    "id": "yz3bjxcbvZ7r"
   },
   "source": [
    "## Data \n",
    "\n",
    "To begin, we will examine `insurance.csv` data set on the medical costs which comes from the [Medical Cost Personal dataset](https://www.kaggle.com/datasets/mirichoi0218/insurance). Our goal is to model the yearly medical charges of an individual using some combination of the other features in the data. The included columns are as follows:\n",
    "\n",
    "* `charges` - yearly medical charges in USD\n",
    "* `age` - the individuals age\n",
    "* `sex` - the individuals sex, either `\"male\"` or `\"female\"`\n",
    "* `bmi` - the body mass index of the individual\n",
    "* `children` - the number of dependent children the individual has\n",
    "* `smoker` - a factor with levels `\"yes\"`, the individual is a smoker and `\"no\"`, the individual is not a smoker\n",
    "\n",
    "We read the data into python using pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF19U6ivvZ7r"
   },
   "outputs": [],
   "source": [
    "df_insurance = pd.read_csv(\"insurance.csv\")\n",
    "df_insurance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izo4A3SSvZ7t"
   },
   "source": [
    "# Exploratory Data Analysis <a id='eda'></a>\n",
    "\n",
    "Before modelling, we will start with EDA to gain an understanding of the data, through descriptive statistics and visualizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-da6d5f4b-bf6b-43f7-9751-a015e6a9924e",
    "deepnote_cell_type": "markdown",
    "id": "50AbKP76vZ7u"
   },
   "source": [
    "### ðŸš© Exercise 1 (CORE)\n",
    "\n",
    "a) Examine the data structure and look at the descriptive statistics. What are the types of variables in the data set? \n",
    "\n",
    "b) Create a pairs plot of the data (make sure to include the `smoker` column), describe any relationships you observe in the data. To better visualize the relationship between `children` and `charges`, create a violin plot (since `children` only takes a small number of integer values, many points are overlaid in the scatterplot and making visualization difficult).\n",
    "\n",
    "\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- <code>.describe()</code> can be used to create summary descriptive statistics on a pandas dataframe.\n",
    "- You can use a <code>sns.pairplot</code> and <code>sns.violinplot</code> with the hue argument\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-4d9907ea-c7c7-4c16-aee7-5e12c3e00cc1",
    "deepnote_cell_type": "code",
    "id": "-mG-rvtrvZ7u",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Part a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-a3d1594c-000b-48dd-b347-dfa5410fe0a7",
    "deepnote_cell_type": "markdown",
    "id": "01HS3aS8vZ7u"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Train-Test Set <a id='gen'></a>\n",
    "\n",
    "Before modelling, we will first split the data into the train and test sets. This ensures that that we do not violate one of the golden rules of machine learning: never use the test set for training. As EDA can help guide the choice and form of model, we also may want to split the data before EDA, to avoid peeking at the test data too much during this phase. However, in practice, we may need to investigate the entire data during EDA to get a better idea on how to handle issues such as missingness, categorical data (and rare categories), incorrect data, etc.      \n",
    "\n",
    "There are lots of ways of creating a test set. We will use a helpful function from `sklearn.model_selection` called `train_test_split`. You can have a look at the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html. Note that `train_test_split` defaults to randomly sampling the data to split it into training and validation/test sets, that is, the default value is `shuffle=True`  \n",
    "\n",
    "For reproducibility, we first fix the value in the numpy random seed about the state of randomness. This ensures that, every step including randomness, will produce the same output if we re-run the code or if someone else wants to reproduce our results (e.g. produce the same train-test split).\n",
    "\n",
    "In `sklearn`, the suggestion to control randomness across multiple consecutive executions is as follows: \n",
    "\n",
    "- In order to obtain reproducible (i.e. constant) results across multiple program executions, we need to remove all uses of `random_state=None`, which is the default.\n",
    "\n",
    "- Declare your own `rng` variable (random number generator) at the top of the program, and pass it down to any object that accepts a `random_state` parameter. You can check some details from here; https://numpy.org/doc/1.16/reference/generated/numpy.random.RandomState.html\n",
    "\n",
    "Thus, our first step before splitting the data is to define our `rng` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make this notebook's output identical at every run\n",
    "rng = np.random.seed(0)\n",
    "# might be good for our course\n",
    "# np.random.seed(11205) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 2 (CORE)\n",
    "\n",
    "Use `train_test_split()` to split the data randomly into training (70%) and test (30%) sets. The `training set` will contain our test data, which you should call `X_train` and `y_train`. The `test set` will contain our testing data, which you should call `X_test` and `y_test`. Don't forget to pass in the `rng` variable to `random_state`.\n",
    "\n",
    "Can you think on a scenario where not shuffling would be a good idea? What about when we would want to shuffle our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split off features and targets\n",
    "X = df_insurance.drop('charges', axis = 1) # Set of features\n",
    "y = df_insurance['charges']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-7d871b29-2a08-4b7f-8421-d36b23056000",
    "deepnote_cell_type": "markdown",
    "id": "f8cnwbvKvZ7v"
   },
   "source": [
    "# Least Squares Estimation <a id='RBH'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-ca8096af-6bc2-4bf1-aea6-19d55952f208",
    "deepnote_cell_type": "markdown",
    "id": "wyQLrGCvvZ7w"
   },
   "source": [
    "Consider a linear regression model for `charges` using `bmi` and `smoker` as features in our model. Without sklearn functionalities, let's compute and visualize the least squares estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-92e05d0b-dedd-444b-a9f3-df423c4b0e70",
    "deepnote_cell_type": "markdown",
    "id": "JdkqCIkUvZ7w"
   },
   "source": [
    "### ðŸš© Exercise 3 (CORE)\n",
    "\n",
    "Create a scatter plot using the `insurance_df` data frame. Describe any apparent relationship between `charges` and `bmi` (including the `smoker` attribute) and comment on the difference between smokers and non-smokers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-abd9df3c-dd0b-43bc-a4b4-4369e7a9cc3e",
    "deepnote_cell_type": "code",
    "id": "UJt4fEW2vZ7w",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 4 (CORE)\n",
    "\n",
    "Now, let's compute the least square estimates.\n",
    "\n",
    "a) First construct the design matrix as an `np.array`. Recall that we need to include a column of ones to allow a non-zero intercept. You will also need to convert the response `y_train` to an `np.array`.\n",
    "\n",
    "b) Compute the least squares estimates $\\hat{\\boldsymbol{w}}$, using the expression from lectures and the `solve` function from `numpy.linalg`.\n",
    "\n",
    "c) What is the intercept for non-smokers and what is the intercept for smokers?\n",
    "\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "Your design matrix should have three columns, with the last column indicating if the individual is a smoker:\n",
    "    $$ \n",
    "x_{n,3} = \n",
    "\\begin{cases}\n",
    "1 & \\text {if individual $n$ is a smoker} \\\\\n",
    "0 & \\text {if individual $n$ is not a smoker}\n",
    "\\end{cases}\n",
    "$$\n",
    "You can create this feature in different ways, for example simply using `X_train.smoker == \"yes\"` (or using `pd.get_dummies` or `OneHotEncoder`).\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b\n",
    "from numpy.linalg import solve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "mvrOq4afvZ7z"
   },
   "source": [
    "### ðŸš© Exercise 5 (CORE)\n",
    "\n",
    "a) Compute the fitted values from this model by calculating $\\hat{\\mathbf{y}} = \\boldsymbol{X} \\hat{\\boldsymbol{w}}$. \n",
    "\n",
    "b) Redraw your scatter plot of `bmi` against `charges`, colored by `smoker` from Exercise 3, and overlay a line plot of the fitted values (fitted regression line). Comment on the results and any potential feature engineering steps that could help to improve the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-04722d32-f420-4104-b25b-37656a71df76",
    "deepnote_cell_type": "code",
    "id": "3Mxs7ORhvZ71",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Part a: Compute fitted values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b: Plot fitted values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uoEYNyw0Jfm"
   },
   "source": [
    "## Residuals\n",
    "\n",
    "A useful tool for evaluating a model is to examine the residuals of that model. For any standard regression model,  the residual for observation $n$ is defined as $y_n - \\hat{y}_n$ where $\\hat{y}_n$ is the model's fiited value for observation $n$. \n",
    "\n",
    "Studying the properties of the residuals is important for assessing the quality of the fitted regression model. This scatterplot (fitted vs residuals) gives us more intuition about the model performance. Briefly, \n",
    "\n",
    ">- If the normal linear model assumption is true then the residuals should be randomly scattered around zero with no discernible clustering or pattern with respect to the fitted values. \n",
    ">- Furthermore, this plot can be useful to check the constant variance (homoscedastic) assumption to see whether the range of the scatter of points is consistent over the range of fitted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 6 (CORE)\n",
    "\n",
    "a) Calculate the residuals and create a residual plot (scatter plot of fitted vs residuals) for this model and color by smoker. Comment on quality of the model based on this plot.\n",
    "\n",
    "b) Compute the $R^2$ value for this model and comment on its value (recall from lectures that $R^2$ is 1 minus the sum of the squared residuals divided by the sum of squared differences between $\\mathbf{y}$ and its mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRdjYEgP1ahd"
   },
   "outputs": [],
   "source": [
    "# Part a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIICk-3l4TVj"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClWZ7isN5ekk"
   },
   "outputs": [],
   "source": [
    "# Part b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank deficiency\n",
    "\n",
    "### ðŸš© Exercise 7 (CORE)\n",
    "\n",
    "Now lets consider the model where we naively include both dummies variables for smokers and non-smokers as well as an intercept column in our model matrix. What happens when you try to compute the least squares estimate in this case? What is the rank of the design matrix? You can use `numpy.linalg.matrix_rank` to compute the rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the least squares estimate when both dummy variable are included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the rank of the design matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-021075dd-c202-41db-9f49-2b4de4efd791",
    "deepnote_cell_type": "markdown",
    "id": "ilhDv3WHvZ73"
   },
   "source": [
    "---\n",
    "# Regression using scikit-Learn <a id='RSKL'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-068e26c7-da60-4c7c-8551-5f914b53af75",
    "deepnote_cell_type": "markdown",
    "id": "NtUQzMD9vZ73"
   },
   "source": [
    "Linear regression is available in **scikit-learn** (**sklearn**) through `LinearRegression` from the `linear_model` submodule. You can browse through the documentation and examples [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Let's start by importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00028-40915b00-d692-4d82-9df3-1a72901e798d",
    "deepnote_cell_type": "code",
    "id": "WBf8UcpzvZ73",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-a3ead172-d8aa-4570-9aae-68cd6d18692d",
    "deepnote_cell_type": "markdown",
    "id": "o2W30cCUvZ73"
   },
   "source": [
    "In general sklearn's models are implemented by first creating a model object, and then using that object to fit your data. As such, we will now create a linear regression model object `lr` and use it to fit our data. Once this object is created we use the `fit` method to obtain a model object fitted to our data. \n",
    "\n",
    "Note that by default an intercept is included in the model. So, we do NOT need to add a column of ones to our design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00030-d0c72235-001d-4cd4-a50f-302b40d9e4d3",
    "deepnote_cell_type": "code",
    "id": "VcuBnGPavZ73",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "X_train_ = np.c_[\n",
    "    X_train.bmi,\n",
    "    X_train.smoker == \"yes\"\n",
    "]\n",
    "\n",
    "lr_fit = lr.fit(\n",
    "    X = X_train_, \n",
    "    y = y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00031-b1d09da5-a8c6-46db-a5ec-617662e344a9",
    "deepnote_cell_type": "markdown",
    "id": "kMZfWvLKvZ74"
   },
   "source": [
    "This model object then has various useful methods and attributes, including `intercept_` and `coef_` which contain our estimates for $\\boldsymbol{w}$. \n",
    "\n",
    "Note that if `fit_intercept=False` and a column of ones is included in the design matrix, then both the intercept and coefficient will be stored in `coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-d4cd3759-a588-4d95-b34e-507416ebc046",
    "deepnote_cell_type": "code",
    "id": "Rt0cuv8uvZ74",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "w_0 = lr_fit.intercept_  # Intercept term of the fitted model\n",
    "w_1 = lr_fit.coef_ \n",
    "\n",
    "w = np.concatenate([[w_0], w_1])\n",
    "print(w)\n",
    "\n",
    "# Or use or helper function to combine the intercept and coefficient into a single array\n",
    "get_coefs(lr_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-9bb64eec-8879-48d5-832a-101c7a3dc752",
    "deepnote_cell_type": "markdown",
    "id": "lLUKEr2YvZ75"
   },
   "source": [
    "The model fit objects also provide additional useful methods for evaluating the model $R^2$ (`score`) and calculating predictions (`predict`). Let's use the latter to compute the fitted values and predictions, as well as some metrics to evaluate the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00036-fdd889c3-0b7b-449e-a815-6e1e055f9703",
    "deepnote_cell_type": "code",
    "id": "rqv-rPpQvZ75",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Fitted values\n",
    "y_fit = lr_fit.predict(X_train_)\n",
    "\n",
    "# Predicted values\n",
    "X_test_ = np.c_[\n",
    "    X_test.bmi,\n",
    "    X_test.smoker == \"yes\"\n",
    "]\n",
    "y_pred = lr_fit.predict(X_test_)\n",
    "\n",
    "# The mean squared error of the training set \n",
    "print(\"Training Mean squared error: %.3f\" % mean_squared_error(y_train, y_fit))\n",
    "# The coefficient of determination of the training set \n",
    "print(\"Training R squared: %.3f\" % r2_score(y_train, y_fit))\n",
    "\n",
    "# The mean squared error of the test set \n",
    "print(\"Test Mean squared error: %.3f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination of the test set \n",
    "print(\"Test R squared: %.3f\" % r2_score(y_test, y_pred))\n",
    "\n",
    "# Another way for R2 calculation\n",
    "print(lr.score(X_train_, y_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use the pre-defined function\n",
    "model_fit(lr_fit, X_train_, y_train, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 8 (CORE)\n",
    "\n",
    "Consider a pipeline for a regression model, using sklearn functionalities \n",
    "\n",
    "- including `bmi` and `age` as numerical values and smoker condition as a categorical value\n",
    "\n",
    "- Apply encoding for the `smoker` variable within the pipeline\n",
    "\n",
    "- Fit a model including these variables and calculate performance metrics similar to above ($R^2$, and MSE / RMSE). How does this model compare to previous one?\n",
    "\n",
    "Note that using the option `OneHotEncoder(drop=np.array(['Reference Category']))`, we can specify the which category to drop (the reference category) by replacing `'Reference Category'` with the desired category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Create ML pipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and calculate metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# Polynomial Regression<a id='polyreg'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial features in sklearn\n",
    "\n",
    "sklearn has a built in function called `PolynomialFeatures` which can be used to simplify the process of including polynomial features in a model. You can browse the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). This function is included in the *preprocessing* module of sklearn, as with other python functions we can import it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction and use of this is similar to what we have already seen with other transformers; we construct a PolynomialFeatures object in which we set basic options (e.g. the degree of the polynomial) and then apply the transformation to our data by calling `fit_transform`. This will generate a new model matrix which includes the polynomial features up to the degree we have specified.\n",
    "\n",
    "Run the following code for a simple illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "PolynomialFeatures(degree = 2).fit_transform(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we use this transformation, we get **all of the polynomial transformations of x from 0 to degree**. \n",
    "\n",
    "In this case, the **0 degree column** is equivalent to **the intercept column**. If we do not want to include this we can construct `PolynomialFeatures` with the option `include_bias=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree = 2, include_bias=False).fit_transform(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `PolynomialFeatures` to add only interaction terms, through the option `interaction_only=True`. As an illustration run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4],[0,0,1,1]]).T\n",
    "PolynomialFeatures(interaction_only=True,include_bias=False).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions\n",
    "\n",
    "Now, let's create a pipeline that:\n",
    "\n",
    "- includes `bmi` as a numerical variable and smoker condition as a categorical variable\n",
    "\n",
    "- applies encoding for the `smoker` variable\n",
    "\n",
    "- uses `PolynomialFeatures` to include an **interaction** between `smoker` and `bmi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline for model that includes interactions\n",
    "cat_pre = OneHotEncoder(drop=np.array(['no']))\n",
    "\n",
    "pf = PolynomialFeatures(interaction_only=True,include_bias=False)\n",
    "\n",
    "# Overall ML pipeline\n",
    "reg_pipe_2 = Pipeline([\n",
    "    (\"pre_processing\", ColumnTransformer([\n",
    "        (\"cat_pre\", cat_pre, [4]), #Â Applied to smoker\n",
    "        (\"num_pre\", 'passthrough', [2])])), #Â Applied to bmi\n",
    "    (\"interact\", pf),\n",
    "    (\"model\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "lr3_fit = reg_pipe_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "- We have set `include_bias=False` as the intercept is included in linear regression by default\n",
    "\n",
    "- The returned object is a `Pipeline` object so it will not provide direct access to step properties, such as the coefficients for the regression model.\n",
    "\n",
    "- If we want access to the attributes or methods of a particular step we need to first access that step using either its name or position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_pipe_2.named_steps['model'].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_pipe_2.steps[2][1].intercept_) # second subset is necessary here because \n",
    "                                       # each step is a tuple of a name and the \n",
    "                                       # model / transform object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the `get_coefs` helper function supplied.\n",
    "\n",
    "We can also extract the **names of the features** using the method `get_feature_names_out()` of the transfomers. Here we need to first extract the names from the first feature engineering step and then pass them to the second step of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the names of the features\n",
    "names_fe1 = reg_pipe_2['pre_processing'].get_feature_names_out()\n",
    "#print(names_fe1)\n",
    "print(reg_pipe_2['interact'].get_feature_names_out(names_fe1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 9 (CORE)\n",
    "\n",
    "For the model trained above (`lr3_fit`):\n",
    "\n",
    "a) What is the intercept and slope for non-smokers and what is the intercept and slope for smokers?\n",
    "\n",
    "b) Compute the fitted values by calling `predict`. Draw the scatter plot of bmi against charges, colored by smoker (from Exercise 3), and overlay a line plot of the fitted values (fitted regression lines).\n",
    "\n",
    "c) How does this model compare to the previous ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore including nonlinearity into the model through a polynomial basis function expansions of `bmi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 10 (CORE)\n",
    "\n",
    "First, suppose you naively apply a polynomial basis function expansion to the feature matrix containing `bmi` and `smoker`: \n",
    "\n",
    "- Run the code below to first create the feature matrix (we standardize `bmi` to simply help with visualization). \n",
    "\n",
    "- Next, create the transformed feature matrix using `PolynomialFeatures` assuming `degree=2`. Print out the first 20 rows of this matrix. \n",
    "\n",
    "- What are the number of features and what does each column represent? Why should we **NOT** use this naive polynomial basis function expansion?\n",
    "\n",
    "<br><br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- Try printing out the names of each feature with the method `get_feature_names_out(['bmi','smoker'])` on your fitted  `PolynomialFeatures`object.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix containing bmi and smoker (note we standardize bmi here, simply to help avoid\n",
    "# printing very large numbers in the exercise)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "bmi_ss = StandardScaler().fit_transform(np.asarray(X_train.bmi).reshape(-1,1))\n",
    "\n",
    "X_ = np.c_[\n",
    "    X_train.smoker == \"yes\",\n",
    "    bmi_ss\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformed feature matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 11 (CORE)\n",
    "\n",
    "Now, let's create a model that allows nonlinearity of `bmi` through polynomial basis function expansion of degree 3 (with no interactions for ease of exposition). \n",
    "\n",
    "Create a new pipeline to construct this model. Train this model and then plot the fitted regression line and compute the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline for model that includes polynomial expansion of bmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and compute and plot fitted values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Order of the Polynomial\n",
    "\n",
    "How can we choose the order of the polynomial? \n",
    "\n",
    "In lecture, we discussed how chosing the degree to be too large can cause over fittting. When we over fit a polynomial regression model, the MSE for the training data will appear to be low which might indicate that the model is a good fit. But, as a result of over fitting, the MSE for the predictions of the unseen test data may begin to increase. However, we can **NOT** use the test to determine the order of the polynomial, so in the following, we explore using cross-validation to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 12 (EXTRA)\n",
    "\n",
    "First, let's compute and plot the **training and test MSE** over a range of degree values. What do you notice about the fit as we increase the polynomial degree? Which degree seems better regarding the changes on training and testing MSE values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning with GridSearchCV\n",
    "\n",
    "If we wish to test over a specific set of parameter values using cross validation we can use the `GridSearchCV` function from the `model_selection` submodule. In this setting, the hyperparamer is actually the degree of the polynomial that we are investigating. \n",
    "\n",
    "This argument is a dictionary containing parameters names as keys and lists of parameter settings to try as values. Since we are using a pipeline, our parameter name will be the name of the pipeline step, `pre_processing`, followed by `__`, (then, the name of next step if applicable, e.g. `poly__` since we are using `ColumnTransformer`), and then the parameter name, `degree`. So for our pipeline the parameter is named `pre_processing_poly__degree`. If you want to list the names of all available parameters you can call the `get_params()` method on the model object, e.g. `polyreg_pipe.get_params()` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pre = OneHotEncoder(drop=np.array(['no']))\n",
    "\n",
    "pf = PolynomialFeatures(include_bias=False)\n",
    "\n",
    "# Overall ML pipeline\n",
    "polyreg_pipe = Pipeline([\n",
    "    (\"pre_processing\", ColumnTransformer([\n",
    "        (\"cat_pre\", cat_pre, [4]), #Â Applied to smoker\n",
    "        (\"poly\", pf, [2])])), #Â Applied to bmi\n",
    "    (\"model\", LinearRegression())])\n",
    "\n",
    "# Parameters for grid search\n",
    "parameters = {\n",
    "    'pre_processing__poly__degree': np.arange(1,10,1)\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state=rng)\n",
    "\n",
    "grid_search = GridSearchCV(polyreg_pipe, parameters, cv = kf, scoring = 'neg_mean_squared_error', return_train_score=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polyreg_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code goes through the process of fitting all $5 \\times 9$ models as well as storing and ranking the results for the requested scoring metric(s). Note that here we have used `neg_mean_squared_error` as our scoring metric which returns the **negative of the mean squared error**. For more on metrics of regression models, please see: https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "\n",
    "- As the name implies this returns the negative of the usual fit metric, this is because sklearn expects to always optimize for the maximum of a score and the model with the largest negative MSE will therefore be the \"best\". \n",
    "\n",
    "- In this workshop we have used MSE as a metric for testing our models. This metric is entirely equivalent to the root mean squared error for purposes of ranking / ordering models (as the square root is a monotonic transformation). \n",
    "- Sometimes the RMSE is prefered as it is more interpretable, because it has the same units as $y$.\n",
    "\n",
    "Once all of the submodels are fit, we can determine the optimal hyperparameter value by accessing the object's `best_*` attributes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best index: \", grid_search.best_index_)\n",
    "print(\"best param: \", grid_search.best_params_)\n",
    "print(\"best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator is stored in the `.best_estimator` attribute. By default, after this model is found, it is retrained on all training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_['model'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fitted values\n",
    "yhat =grid_search.predict(X_train)\n",
    "\n",
    "# Plot fitted values\n",
    "ax = sns.scatterplot(x = X_train.bmi, y = y_train, hue = X_train.smoker)\n",
    "sns.lineplot(x = X_train.bmi, y = yhat, hue = X_train.smoker, ax=ax, legend = False)\n",
    "ax.set(ylabel='charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validated scores are stored in the attribute `cv_results_`. This contains a number of results related to the grid search and cross-validation. We can convert it into a pandas data frame to view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also recommend to plot the CV scores. Although the grid search may report a best value for the parameter corresponding to the maximum CV score (e.g. min CV MSE), if the curve is relatively flat around the minimum, we may prefer the simpler model.\n",
    "\n",
    "Note in this case, I have also used the option `return_train_score=True` in `GridSearchCV()`, in order to save also the training scores. As expected training MSE decreases when increasing the degree of the polynomial, but the CV MSE has more of a U-shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = np.arange(1,10,1)\n",
    "fig, ax = plt.subplots(figsize=(9,7), ncols=1, nrows=1)\n",
    "plt.scatter(degree,-grid_search.cv_results_['mean_train_score'], color='k')\n",
    "plt.plot(degree,-grid_search.cv_results_['mean_train_score'], color='k', label='Mean Train MSE')\n",
    "plt.scatter(degree,-grid_search.cv_results_['mean_test_score'], color='r')\n",
    "plt.plot(degree,-grid_search.cv_results_['mean_test_score'], color='r', label='CV MSE')\n",
    "ax.legend()\n",
    "ax.set_xlabel('degree')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš© Exercise 13 (EXTRA)\n",
    "\n",
    "Try an alternative model of your choice. What have you chosen and why? Are there any paramters to tune?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further resources\n",
    "\n",
    "- About common pitfalls and interpreting coefficients: \n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name.\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF. Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week04.ipynb "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Sara Wade"
   },
   {
    "name": "Ozan Evkaya"
   }
  ],
  "colab": {
   "collapsed_sections": [],
   "name": "week05.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "2f0f4e4a-50b4-476a-ac32-ea3a1e98d30c",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "title": "MLPy Workshop 4: Solutions"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
